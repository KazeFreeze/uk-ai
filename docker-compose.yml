services:
  # 1. Ollama AI Service
  ollama:
    image: ollama/ollama
    container_name: ollama_service
    restart: unless-stopped
    ports:
      - "11435:11434" # Expose on port 11435 for local dev access
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: /bin/sh
    command: -c "ollama serve & sleep 5 && ollama pull llava:7b && wait"

  # 2. Your Next.js App
  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: nextjs_app
    restart: unless-stopped
    environment:
      - NODE_ENV=development
      # Internal docker network connection
      - OLLAMA_HOST=http://ollama:11434
      - PORT=3000
    ports:
      - "3000:3000"
    volumes:
      - .:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      ollama:
        condition: service_started

volumes:
  ollama_data:
